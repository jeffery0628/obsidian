---
Create: 2022年 一月 16日, 星期日 12:37
tags: 
  - Engineering/hadoop
  - 大数据
---

HDFS中的文件在物理上是分块存储，块的大小可以通过配置参数来规定，默认大小在Hadoop 2.x版本中是128M，老版本中是6M。
![[700 Attachments/Pasted image 20220116123955.png]]


## 思考：为什么块的大小不能设置太小，也不能设置太大？
1. HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；
2. 如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。

> 总结：HDFS块的大小设置主要取决于磁盘传输速率







